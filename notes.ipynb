{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py\n",
    "- store all data structures appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBREF\n",
    "\n",
    "use season as an index so I can compare stats across players for a certain season. also, trends in stats over seasons.  \n",
    "use pandas to get tables.  \n",
    "\n",
    "\n",
    "\n",
    "Scrape mouseover explanations for column names\n",
    "\n",
    "#### **Table Schemas**\n",
    "##### Player info\n",
    "- player id (index)\n",
    "- full name\n",
    "- personal details\n",
    "- club\n",
    "- news json (from Wikipedia)\n",
    "- fbref link (or just save urls_dict as a df)\n",
    "\n",
    "\n",
    "##### Outfielder Stats\n",
    "- player id\n",
    "- team\n",
    "- stats\n",
    "\n",
    "##### Goalie Stats\n",
    "- player id\n",
    "- team\n",
    "- stats\n",
    "\n",
    "\n",
    "\n",
    "##### Scouting report\n",
    "\n",
    "#### <s>**Spider**\n",
    "\n",
    "1. crawl league for team links\n",
    "2. crawl team for player links - put in dict and create player_id index for df\n",
    "3. scrape player page \n",
    "    - headshot\n",
    "    - personal info\n",
    "    - stats\n",
    "    store with id in df or with folder names as id for images.\n",
    "\n",
    "##### Spider class:\n",
    "- attributes\n",
    "    - url list (or dict?)\n",
    "    - max threads\n",
    "    \n",
    "- methods\n",
    "    - crawl list of urls\n",
    "    - parse function\n",
    "    - return result\n",
    "    - save result\n",
    "\n",
    "- refactor imports between abstract_scraper and fbref_scraper. (import all of abstract_scraper instead of just AbstractScraper)</s>\n",
    "- refactor run and crawl methods so that run can remain untouched but it can be called from another method like execute() which can add other functionality such as compiling results into a df or saving them into a file. \n",
    "\n",
    "\n",
    "- <s>sort out dict.keys() vs list in PlayerStatsScraper\n",
    "- sort out urls setters and getters in PlayerStatsScraper. \n",
    "\n",
    "##### Cleaning and storing (infobox):\n",
    "- create player id\n",
    "\n",
    "##### Cleaning and storing (personal Info and stats):\n",
    "- for stats tables (ones with 'stats' in table_dict value):\n",
    "    - <s>drop \"matches\" column</s>\n",
    "    - <s>remove multi-indexing from the relevant columns</s>\n",
    "    - split out leading digits from 'comp' column\n",
    "    - <s>stop rows after seasons end and drop the ones that are team aggregates (find first row in 'Season' with 'seasons' and drop it and all subsequent rows)</s>\n",
    "- <s>join stats tables by rows so that they align by season. When doing that drop the leading left columns (common ones) from all but the first.\n",
    "is there any need to know which table is which? as long as the stats can be accessed by multi-indexed columns?</s>\n",
    "\n",
    "- sort out full name or drop it.\n",
    "- sort out twitter handle parser\n",
    "- add parser for position\n",
    "- add player id</s>\n",
    "- add feature for goalkeepers. make a different table for them. \n",
    "- make check for empty dataframe typesafe (instad of using ```if self.stats_df == \"Empty\"```) \n",
    "\n",
    "##### fetch and store headshots\n",
    "\n",
    "#### **Example queries**\n",
    "- which player in england had the greatest expected goals per 90 min?\n",
    "- which players under 25 scored more than 20 goals in 2020/21?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "\n",
    "Identity resolution possibilites\n",
    "- use full_name along with team name to search Wikipedia.\n",
    "- search name, team, \"footballer\" wikipedia on google and get first link? \n",
    "    - try bing or ddg\n",
    "    - try their APIs?\n",
    "    - use re to search for correct wikipedia link in text  \n",
    "\n",
    "_DDG_\n",
    "- example api search https://api.duckduckgo.com/?q=cristiano%20ronaldo&format=json&pretty=1\n",
    "- if Abstract URL contains wikipedia and soup contains footballer (adjusted for case) then use abstract URL. else report issue.\n",
    "- if \"FirstURL\" : doesn't contain 'born', use \"AbstractURL\"\n",
    "- if all else fails: https://pypi.org/project/duckduckgo2/\n",
    "- to make sure you have the right page, you can \n",
    "\n",
    "decide between selenium/splash/playwright  \n",
    "implement search\n",
    "\n",
    "<s>### Infobox\n",
    "- scrape infobox\n",
    "- store images\n",
    "- store data in PG </s>\n",
    "\n",
    "\n",
    "#### Images\n",
    "\n",
    "\n",
    "<s>#### Data\n",
    "\n",
    "one approach is to loop through children of `soup.find(class_=\"infobox vcard\").tbody()` and \n",
    "- if class is `\"infobox-header\"` then use it as a key\n",
    "- if class is `\"infobox-label\"` then use it as a key within that\n",
    "- if class is `\"infobox-data\"` then store it as data \n",
    "\n",
    "another is to scan through `soup.find(class_=\"infobox vcard\").tbody()` and look for specific items such as \n",
    "- full name,\n",
    "- date of birth,\n",
    "- etc.,  \n",
    "\n",
    "eg. find `infobox header` = personal information. then use function to retrieve personal information\n",
    "- cycle down `<tr>` till you reach another `infobox header\n",
    "\n",
    "in descendants. for boxes with more than one column (eg. careers)\n",
    "\n",
    "### Body\n",
    "- store headings:text in json\n",
    "- get locations of all the h2/h3/paragraph elements in element list of body and then construct tree?\n",
    "\n",
    "Store the whole thing in MongoDB or Postgres JSON field\n",
    "\n",
    "The body is organised using `<h>` tags. </s>\n",
    "\n",
    "#### **Spider**\n",
    "- get names, full names, teams, player_id from postgres table or fbref scraper instance\n",
    "\n",
    "##### Items and pipelines\n",
    "- populate items.py file for \n",
    "    - tuple/json object\n",
    "    - images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESPN\n",
    "figure out if there's a way to do async better with playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General points\n",
    "\n",
    "- make functions of all snippet routines so snippets can be imported as a parser module.\n",
    "- docstring all functions and module\n",
    "- make package for body-text extraction for wikipedia.\n",
    "- <s>make common function for image extraction.</s>\n",
    "- when writing up the project for the github readme, incude \n",
    "    - tree\n",
    "    - screenshots of results at various stages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General resources\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
